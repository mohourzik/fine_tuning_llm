{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependecies\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from trl import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_prompts(example):\n",
    "    fomated_texts = []\n",
    "    for text in example:\n",
    "        formated_text = re.sub(r'\\s*### Answer.*', '', text, flags=re.DOTALL).strip()\n",
    "        fomated_texts.append(formated_text)\n",
    "    return fomated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path_dir = '/teamspace/studios/this_studio/Fine_tuning'\n",
    "\n",
    "dataset = datasets.load_from_disk(path_dir + '/dataset')\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_test_dataset = format_test_prompts(test_dataset['text'])\n",
    "test_dataset = pd.DataFrame(formated_test_dataset, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.Dataset.from_pandas(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Question: Can you provide a program that converts Fahrenheit to Celsius?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['text'][6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_4_bit = True\n",
    "if quant_4_bit:\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "else:\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "max_sequence_length = 1024 # gpt2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'openai-community/gpt2',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "## pad the sequence if it is < max_sequence_length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "tokenizer.model_max_length = max_sequence_length\n",
    "\n",
    "tokenizer.truncation_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.37 MB\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model_name = path_dir + '/gpt2_fine_tuned_model'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        'openai-community/gpt2',\n",
    "        quantization_config = quant_config,\n",
    "        device_map = 'auto',\n",
    "        # local_files_only = True,\n",
    ")\n",
    "fine_tuned_model_loaded = PeftModel.from_pretrained(model, fine_tuned_model_name)\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"{(fine_tuned_model_loaded.get_memory_footprint() / 1e6):.2f} MB\")\n",
    "\n",
    "# 134.06 MB = using quant_4_bit\n",
    "# 176.53 MB = using quant_8_bit\n",
    "# 510.34 MB = without quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(prompt):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    attention_mask = torch.ones(inputs.shape, device='cuda')\n",
    "    output = fine_tuned_model_loaded.generate(inputs, attention_mask=attention_mask, max_new_tokens=200, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python program to find the factorial of a number provided by the user.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Write a Python program to find the factorial of a number provided by the '\n",
      " \"user. Here's an example code snippet:\\n\"\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def factorial(n):\\n'\n",
      " '    if n == 0:\\n'\n",
      " '        return 1\\n'\n",
      " '    else:\\n'\n",
      " '        return n * factorial(n-1)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'In this code, the `factorial` function takes a parameter `n`, which '\n",
      " 'represents the number you want to find the factorial of. It then checks if '\n",
      " '`n` is equal to 0. If it is, it returns 1, otherwise it returns 0.\\n'\n",
      " '\\n'\n",
      " 'To use this function, simply call it and pass the desired number as an '\n",
      " 'argument. For example, if you want to find the factorial of 5, you can do:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'factorial(5)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'The function will then return the factorial of 5, which is 1.\\n'\n",
      " '\\n'\n",
      " 'Feel free to modify the code according to your')\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "prompt = 'Write a Python program to find the factorial of a number provided by the user.'\n",
    "print(prompt)\n",
    "response = model_predict(prompt)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python program to find the sum of the list of random numbers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Write a Python program to find the sum of the list of random numbers. Here's \"\n",
      " 'an example code snippet:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def find_sum(nums):\\n'\n",
      " '    sum = 0\\n'\n",
      " '    for num in nums:\\n'\n",
      " '        sum += num\\n'\n",
      " '    return sum\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'In this code, the `find_sum` function takes a list of numbers as input. It '\n",
      " 'initializes a variable `sum` to 0. Then, it iterates through each number in '\n",
      " 'the list using a `for` loop. Inside the loop, it adds each number to the '\n",
      " '`sum` variable. Finally, it returns the `sum` as the result.\\n'\n",
      " '\\n'\n",
      " 'To use this function, you can simply call it and pass your desired list of '\n",
      " 'numbers as an argument. For example, `find_sum(5, 10)` would return the sum '\n",
      " 'of 5 and 10.\\n'\n",
      " '\\n'\n",
      " 'Feel free to modify the code to suit your specific requirements. Let me know '\n",
      " 'if you have')\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "prompt = 'Write a Python program to find the sum of the list of random numbers.'\n",
    "print(prompt)\n",
    "response = model_predict(prompt)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a list of random numbers between 1 and 100.\n",
      "('Give me a list of random numbers between 1 and 100.\\n'\n",
      " '\\n'\n",
      " \"Here's an example code snippet that demonstrates how to generate random \"\n",
      " 'numbers between 1 and 100:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'random_num = random.randint(1, 100)\\n'\n",
      " 'print(random_num)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'In this code, we first define a list called `random_num` containing the '\n",
      " 'numbers 1, 100, and 1. Then, we use the `random.randint()` function to '\n",
      " 'generate a random integer between 1 and 100. Finally, we print the generated '\n",
      " 'random number using the `print()` function.\\n'\n",
      " '\\n'\n",
      " 'When you run this code, it will output the following:\\n'\n",
      " '\\n'\n",
      " '```\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n'\n",
      " '1\\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "prompt = 'Give me a list of random numbers between 1 and 100.'\n",
    "print(prompt)\n",
    "response = model_predict(prompt)\n",
    "pprint.pprint(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
