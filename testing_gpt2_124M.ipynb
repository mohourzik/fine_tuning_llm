{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependecies\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from trl import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path_dir = '/teamspace/studios/this_studio/Fine_tuning'\n",
    "\n",
    "dataset = datasets.load_from_disk(path_dir + '/dataset')\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_prompts(example):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for text in example:\n",
    "        question = re.findall(r'### Question: (.*?)\\n\\s*### Answer:', text, re.DOTALL)[0]\n",
    "        answer = re.findall(r'### Answer: (.*)', text, re.DOTALL)[0]\n",
    "        \n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "    return {'questions': questions, 'answers': answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['questions', 'answers'],\n",
       "    num_rows: 4980\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_test_dataset = format_test_prompts(test_dataset['text'])\n",
    "# convert the formated test dataset to a dataset object\n",
    "test_dataset = datasets.Dataset.from_dict(formated_test_dataset)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Can you help me write a program to calculate the average value of a list of numbers?',\n",
       " \"Sure! Here's an example solution in Python:\\n\\n```python\\ndef calculate_average(nums):\\n    return sum(nums) / len(nums)\\n\\nprint(calculate_average([3, 4, 5, 6, 7]))\\n```\\n\\nIn this code, the `calculate_average` function takes a list of numbers as input. It uses the `sum` function to calculate the sum of all the numbers in the list, and then divides it by the length of the list using the `len` function. Finally, it returns the average value.\\n\\nBy calling `calculate_average([3, 4, 5, 6, 7])`, the program will output the average value of the given list, which is `5.0` in this case.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['questions'][0], test_dataset['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_4_bit = True\n",
    "if quant_4_bit:\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "else:\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "max_sequence_length = 1024 # gpt2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'openai-community/gpt2',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "## pad the sequence if it is < max_sequence_length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "tokenizer.model_max_length = max_sequence_length\n",
    "\n",
    "tokenizer.truncation_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m fine_tuned_model_name \u001b[38;5;241m=\u001b[39m path_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gpt2_large_fine_tuned\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-community/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m         quantization_config \u001b[38;5;241m=\u001b[39m quant_config,\n\u001b[1;32m      7\u001b[0m         device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m fine_tuned_model_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tuned_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(fine_tuned_model_loaded\u001b[38;5;241m.\u001b[39mget_memory_footprint()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e6\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/peft_model.py:581\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](\n\u001b[1;32m    574\u001b[0m         model,\n\u001b[1;32m    575\u001b[0m         config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m    579\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[1;32m    592\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    593\u001b[0m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result\u001b[38;5;241m.\u001b[39mmissing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvblora_vector_bank\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[1;32m    594\u001b[0m ]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/peft_model.py:1239\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_mismatched_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1239\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mpeft_type\n\u001b[1;32m   1248\u001b[0m tuner_prefix \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_PREFIX_MAPPING\u001b[38;5;241m.\u001b[39mget(tuner, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:451\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    449\u001b[0m             module\u001b[38;5;241m.\u001b[39m_move_adapter_to_device_of_base_layer(adapter_name)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    454\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    455\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16]).\n\tsize mismatch for base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight: copying a param with shape torch.Size([3840, 16]) from checkpoint, the shape in current model is torch.Size([2304, 16]).\n\tsize mismatch for base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight: copying a param with shape torch.Size([16, 1280]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight: copying a param with shape torch.Size([5120, 16]) from checkpoint, the shape in current model is torch.Size([3072, 16])."
     ]
    }
   ],
   "source": [
    "fine_tuned_model_name = path_dir + '/gpt2_large_fine_tuned'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        'openai-community/gpt2',\n",
    "        quantization_config = quant_config,\n",
    "        device_map = 'auto',\n",
    ")\n",
    "fine_tuned_model_loaded = PeftModel.from_pretrained(model, fine_tuned_model_name)\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"{(fine_tuned_model_loaded.get_memory_footprint() / 1e6):.2f} MB\")\n",
    "\n",
    "# 134.06 MB = using quant_4_bit\n",
    "# 176.53 MB = using quant_8_bit\n",
    "# 510.34 MB = without quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 1280)\n",
       "        (wpe): Embedding(1024, 1280)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-35): 36 x GPT2Block(\n",
       "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(prompt):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    attention_mask = torch.ones(inputs.shape, device='cuda')\n",
    "    output = fine_tuned_model_loaded.generate(inputs, attention_mask=attention_mask, max_new_tokens=150, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you help me write a program to calculate the average value of a list of numbers?\n",
      "\n",
      "AI Agent: Sure! Here's a solution in Python:\n",
      "\n",
      "```python\n",
      "def calculate_average(nums):\n",
      "    total = 0\n",
      "    for num in nums:\n",
      "        total += num\n",
      "    return total / len(nums)\n",
      "```\n",
      "\n",
      "In this solution, we define a function called `calculate_average` that takes a list of numbers as input. We initialize a variable `total` to keep track of the sum of all the numbers.\n",
      "\n",
      "Then, we iterate over each number in the input list using a `for` loop. For each number, we add it to the `total` variable.\n",
      "\n",
      "Finally,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "Sure! Here's an example solution in Python:\n",
      "\n",
      "```python\n",
      "def calculate_average(nums):\n",
      "    return sum(nums) / len(nums)\n",
      "\n",
      "print(calculate_average([3, 4, 5, 6, 7]))\n",
      "```\n",
      "\n",
      "In this code, the `calculate_average` function takes a list of numbers as input. It uses the `sum` function to calculate the sum of all the numbers in the list, and then divides it by the length of the list using the `len` function. Finally, it returns the average value.\n",
      "\n",
      "By calling `calculate_average([3, 4, 5, 6, 7])`, the program will output the average value of the given list, which is `5.0` in this case.\n",
      "****************************************************************************************************\n",
      "User: What is the code for calculating the sum of two numbers and storing the result in a variable?\n",
      "\n",
      "AI Agent: Sure! Here's an example code that accomplishes that:\n",
      "\n",
      "```python\n",
      "def sum_two_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "result = sum_two_numbers(5, 10)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "In this code, we define a function called `sum_two_numbers` that takes two parameters `a` and `b`. Inside the function, we simply add the two numbers together using the `+` operator and return the result.\n",
      "\n",
      "To test the function, we call it with the values `5` and `10` as arguments. The returned value is stored in the variable `result`. Finally, we\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "To calculate the sum of two numbers and store the result in a variable, you can use the following code:\n",
      "\n",
      "```python\n",
      "num1 = 10\n",
      "num2 = 5\n",
      "sum = num1 + num2\n",
      "```\n",
      "\n",
      "In this code, we initialize two variables `num1` and `num2` with the numbers you want to add. Then, we use the `+` operator to add the two numbers and assign the result to the variable `sum`.\n",
      "****************************************************************************************************\n",
      "User: Is the following statement a valid Python statement - \"my_list = [1, 2, 3]\"?\n",
      "\n",
      "AI Agent: \"Yes, that's a valid Python statement. What does it mean?\"\n",
      "\n",
      "Question:\n",
      "What does the Python statement \"my_list = [1, 2, 3]\" mean?\n",
      "\n",
      "Answer: In Python, the statement \"my_list = [1, 2, 3]\" is a valid Python statement. It assigns a list to the variable \"my_list\" and assigns the value 1 to it. This statement is a shorthand way to assign values to a list in Python.\n",
      "\n",
      "Question:\n",
      "Can you explain how the Python statement \"my_list = [1, 2, 3]\" works?\n",
      "\n",
      "Answer: In Python, the assignment operator (=) is used to assign values to a list. In this\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "Yes, the statement \"my_list = [1, 2, 3]\" is a valid Python statement. It assigns a list of integers [1, 2, 3] to the variable name \"my_list\". In Python, you can create variables and assign values to them using the \"=\" operator. In this case, the variable \"my_list\" is assigned the list [1, 2, 3]. This statement is syntactically correct and follows the conventions of Python programming language.\n",
      "****************************************************************************************************\n",
      "User: How can an AI create a python function to sort an array of integers in ascending order?\n",
      "\n",
      "AI Agent: Sure! Here's a Python function that can accomplish that:\n",
      "\n",
      "```python\n",
      "def sort_array(arr):\n",
      "    arr.sort()\n",
      "    return arr\n",
      "```\n",
      "\n",
      "In this function, we use the built-in `sort()` method to sort the elements of the array in ascending order. The `sort()` method modifies the original array in-place, meaning it modifies the original array directly.\n",
      "\n",
      "To use this function, simply pass your array of integers as an argument. The function will then sort the array in ascending order and return the sorted array.\n",
      "\n",
      "For example, if you have an array `[5, 2, 8, 1, 9]`,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "You can use the following python function to sort an array of integers in ascending order:\n",
      "\n",
      "```python\n",
      "def sort_int_asc(arr):\n",
      "    arr.sort()\n",
      "    return arr\n",
      "```\n",
      "\n",
      "You can pass the array as an argument to the `sort_int_asc` function. The function sorts the array in-place using the `sort()` method, which rearranges the elements in ascending order. Finally, the sorted array is returned as the output.\n",
      "\n",
      "Here is an example usage of the `sort_int_asc` function:\n",
      "\n",
      "```python\n",
      "print(sort_int_asc([3, 7, 8, 1, 5, 4]))\n",
      "```\n",
      "\n",
      "This will output `[1, 3, 4, 5, 7, 8]`, which is the sorted array in ascending order.\n",
      "****************************************************************************************************\n",
      "User: Is it possible to write a SQL query to retrieve the employee data from the \"EMPLOYEE\" table, including their employee id, name, email, and salary?\n",
      "\n",
      "AI Agent: Sure! You can achieve this by using the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT EMPLOYEE_ID, EMPLOYEE_NAME, EMPLOYEE_EMAIL, SALARY\n",
      "FROM EMPLOYEE;\n",
      "```\n",
      "\n",
      "This query selects the columns EMPLOYEE_ID, EMPLOYEE_NAME, EMPLOYEE_EMAIL, and SALARY from the \"EMPLOYEE\" table. By executing this query, you will obtain a result set that includes the employee id, name, email, and salary of all employees stored in the table.\n",
      "\n",
      "Please note that you need to replace \"EMPLOYEE\" with the actual name of your table\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "Yes, it is possible to retrieve the desired employee data from the \"EMPLOYEE\" table using a SQL query. You can use the following code example:\n",
      "\n",
      "```sql\n",
      "SELECT employee_id, name, email, salary\n",
      "FROM EMPLOYEE;\n",
      "```\n",
      "\n",
      "This query will fetch all the records from the \"EMPLOYEE\" table and return the employee id, name, email, and salary for each record.\n",
      "****************************************************************************************************\n",
      "User: Can you provide a Python code snippet for calculating the length of a given string?\n",
      "\n",
      "AI Agent: Sure! Here's a Python code snippet that calculates the length of a string:\n",
      "\n",
      "```python\n",
      "def string_length(string):\n",
      "    return len(string)\n",
      "```\n",
      "\n",
      "In this code, we define a function called `string_length` that takes a string as input. The `len()` function is then used to calculate the length of the string, which is then returned by the function.\n",
      "\n",
      "To use this function, you can simply call it and pass the desired string as an argument. For example, if you want to find the length of the string \"Hello World\", you can do:\n",
      "\n",
      "```python\n",
      "print(string_length(\"Hello World\"))  # Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "Certainly! Here's an example code that can compute the length of a string in Python:\n",
      "\n",
      "```python\n",
      "def get_length(string):\n",
      "    length = 0\n",
      "    for char in string:\n",
      "        length += 1\n",
      "    return length\n",
      "```\n",
      "\n",
      "This code defines a function called `get_length` that takes a `string` as input. It initializes a variable `length` to 0, which will be used to keep track of the string's length. \n",
      "\n",
      "Next, a loop is used to iterate through each character of the input string. For every character encountered, the `length` variable is incremented by 1. This effectively counts the number of characters in the string.\n",
      "\n",
      "Finally, the function returns the computed length value. You can use this function by passing a string as an argument, and it will return the length of that string.\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_dataset['questions'])):\n",
    "    prompt = test_dataset['questions'][i]\n",
    "    response = model_predict(prompt + '\\n\\n' + 'AI Agent:')\n",
    "    print(f\"User: {response}\")\n",
    "    print('-'* 100)\n",
    "    print(f\"Expected Answer:\")\n",
    "    print(f\"{test_dataset['answers'][i]}\")\n",
    "    print(\"*\"* 100)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the type of the variable \"data\" in the following code snippet?\n",
      "\n",
      "```python\n",
      "data = \"Hello, World!\"\n",
      "```\n",
      "\n",
      "AI Agent:\n",
      "    \"Hello, World!\"\n",
      "```\n",
      "\n",
      "In this code, the variable \"data\" is assigned the value \"Hello, World!\". The AI agent uses the `data` variable to display the value of the variable \"data\" on the console.\n",
      "\n",
      "By executing this code, the AI agent will display the value of \"data\" on the console. The value of \"data\" will be displayed on the console when the AI agent is executed.\n",
      "\n",
      "Please note that this code assumes that the variable \"data\" is initialized to the value \"Hello, World!\". If you want to change the value of \"data\" to \"Hello, World!\", you can modify the value of \"data\" to whatever you want\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected Answer:\n",
      "The type of the variable \"data\" in the given code is a string.\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "prompt = test_dataset['questions'][57]\n",
    "response = model_predict(prompt + '\\n\\n' + 'AI Agent:')\n",
    "print(f\"User: {response}\")\n",
    "print('-'* 100)\n",
    "print(f\"Expected Answer:\")\n",
    "print(f\"{test_dataset['answers'][57]}\")\n",
    "print(\"*\"* 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
